{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQl6BxbTv-eK"
      },
      "source": [
        "# Activations\n",
        "\n",
        "В этой тетрадке мы напишем собственную реализацию функций активации\n",
        "\n",
        "\n",
        "Запрещено внутри своей реализации создавать класс активации из pytorch и просто применять его. Разрешено исползовать простые ф-ии pytorch типа [torch.exp](https://pytorch.org/docs/stable/generated/torch.exp.html) и т д\n",
        "\n",
        "Если у ф-ии активации есть дополнительные аргументы, значение по умолчанию должно быть такое же как в реализации PyTorch\n",
        "\n",
        "**Материалы по pytorch:**\n",
        "\n",
        "* [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNMlqoCiv-eM"
      },
      "source": [
        "## Prerequirements\n",
        "\n",
        "```\n",
        "pip install torch numpy\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-24T12:02:03.697132Z",
          "iopub.status.busy": "2021-01-24T12:02:03.696647Z",
          "iopub.status.idle": "2021-01-24T12:02:05.150001Z",
          "shell.execute_reply": "2021-01-24T12:02:05.148949Z",
          "shell.execute_reply.started": "2021-01-24T12:02:03.697089Z"
        },
        "id": "ovtEtMOxv-eN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGTRWWFo5pOy"
      },
      "source": [
        "## Задание 1\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9rZXbfck6WPO"
      },
      "outputs": [],
      "source": [
        "# Из ячейки не нужно убирать или добавлять другие импорты или объединять с тдругими ячейками\n",
        "# Это может сломать автотесты\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyReLU(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        # hint! Входной тензор нужно скопировать,\n",
        "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
        "        input_clone = torch.clone(input)\n",
        "        # ...\n",
        "        input_clone[input_clone<0] = 0\n",
        "        # return ...\n",
        "        return input_clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRV-Ea2N5zDb"
      },
      "source": [
        "## Задание 2\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html#torch.nn.LeakyReLU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1ouIrrlq63uN"
      },
      "outputs": [],
      "source": [
        "# Из ячейки не нужно убирать или добавлять другие импорты или объединять с тдругими ячейками\n",
        "# Это может сломать автотесты\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyLeakyReLU(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        # hint! Входной тензор нужно скопировать,\n",
        "        # чтобы действиями внутри этого метода не привели к изменению внешнего аргумента\n",
        "        input_clone = torch.clone(input)\n",
        "        input_clone[input_clone<0] *= 0.01\n",
        "        # return ...\n",
        "        return input_clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS2RXTz_55iS"
      },
      "source": [
        "## Задание 3\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QTHnuZQA67ga"
      },
      "outputs": [],
      "source": [
        "# Из ячейки не нужно убирать или добавлять другие импорты или объединять с тдругими ячейками\n",
        "# Это может сломать автотесты\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MySigmoid(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        # для этого класса копировать входной тензор уже нет необходимости, почему?\n",
        "        sig_func = 1/(1 + torch.exp(-input))\n",
        "        return sig_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZs035Rv-eS"
      },
      "source": [
        "## Задание 4\n",
        "**(0.25 балла)** Напишите свою версию версию функции активации [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html#torch.nn.ELU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CT1IEmzFv-eT"
      },
      "outputs": [],
      "source": [
        "# Из ячейки не нужно убирать или добавлять другие импорты или объединять с тдругими ячейками\n",
        "# Это может сломать автотесты\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyELU(nn.Module):\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_clone = torch.clone(input)\n",
        "        alpha = 1.0\n",
        "        input_clone[input_clone <= 0] = alpha * (torch.exp(input_clone[input_clone <= 0]) - 1)\n",
        "        return input_clone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUfu8arDKoNm"
      },
      "source": [
        "## Тест\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uAagGpwSKqBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09eae75-cd64-4cf5-fba4-b7ce9f2d7d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyReLU()\n",
            "MyLeakyReLU()\n",
            "MySigmoid()\n",
            "MyELU()\n"
          ]
        }
      ],
      "source": [
        "import pytest\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _check_pytorch_module_was_not_used(file, module):\n",
        "\n",
        "    file = open(file, mode='r')\n",
        "\n",
        "    assert module not in file.read(), \"pytorch module must not be used in you activation implementation\"\n",
        "\n",
        "    file.close()\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def _test_activation(myactivation, torch_activation):\n",
        "    print(myactivation)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        randinput = torch.rand([100])\n",
        "        myactivation_output = myactivation(randinput)\n",
        "\n",
        "        assert id(myactivation_output) != id(randinput), 'pytorch activation function must return new tensor'\n",
        "\n",
        "        for _ in range(100):\n",
        "            randinput = torch.rand([5, 5, 5])\n",
        "\n",
        "            assert torch.allclose(myactivation(randinput), torch_activation(randinput)), 'activation output is not equals to touch ones output'\n",
        "\n",
        "def test_relu():\n",
        "\n",
        "    my_activation = MyReLU()\n",
        "\n",
        "    #_check_pytorch_module_was_not_used(\"myrelu.py\", '.ReLU(')\n",
        "    _test_activation(my_activation, nn.ReLU())\n",
        "\n",
        "def test_leaky_relu():\n",
        "\n",
        "    my_activation = MyLeakyReLU()\n",
        "\n",
        "    #_check_pytorch_module_was_not_used(\"myleakyrelu.py\", '.LeakyReLU(')\n",
        "    _test_activation(my_activation, nn.LeakyReLU())\n",
        "\n",
        "def test_sigmoid():\n",
        "\n",
        "    my_activation = MySigmoid()\n",
        "\n",
        "    #_check_pytorch_module_was_not_used(\"mysigmoid.py\", '.MySigmoid(')\n",
        "    _test_activation(my_activation, nn.Sigmoid())\n",
        "\n",
        "def test_elu():\n",
        "\n",
        "    my_activation = MyELU()\n",
        "\n",
        "    #_check_pytorch_module_was_not_used(\"myelu.py\", '.MyELU(')\n",
        "    _test_activation(my_activation, nn.ELU())\n",
        "\n",
        "\n",
        "test_relu()\n",
        "test_leaky_relu()\n",
        "test_sigmoid()\n",
        "test_elu()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "bNMlqoCiv-eM"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "toc-autonumbering": false
  },
  "nbformat": 4,
  "nbformat_minor": 0
}